# !pip install pytest-warnings==0.3.1
# !pip install numpy==1.25.1
# !pip install pandas==2.0.3
# !pip install matplotlib=3.7.2
# !pip install seaborn==0.12.2
# !pip install scikit-learn==1.3.0
#set ignore warnings
import warnings
warnings.filterwarnings("ignore")
#import libs for data handling
import numpy as np
import pandas as pd
pd.set_option("display.max_columns", None)
pd.set_option("display.max_rows", None)

# @title Step-1: Image Dataset and data inspection

from google.colab import files
uploaded = files.upload()
housing = pd.read_csv("train.csv")
housing.shape
housing.info
housing.isnull().sum()/housing.shape[0]*100

# @title Step 2: Data Cleaning

cols = ["Alley", "BsmtQual",	"BsmtCond",	"BsmtExposure",	"BsmtFinType2",	"FireplaceQu",	"GarageType",	"GarageFinish",	"GarageQual", "GarageCond",	"PoolQC",	"Fence",	"MiscFeature"]
cols
for features in cols:
  housing[features].fillna("None", inplace = True)
housing.isnull().sum()/housing.shape[0]*100
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
plt.figure(figsize=[6,6])
sns.distplot(housing['SalePrice'])
plt.show()
housing['SalePrice'].skew()
housing['SalePrice'].kurt()
housing.drop("Id", axis=1, inplace=True)
housing[['MSSubClass', 'OverallQual', 'OverallCond']] = housing[['MSSubClass', 'OverallQual', 'OverallCond']].astype('object')housing['MasVnrArea'] = pd.to_numeric(housing['MasVnrArea'], errors='coerce')
housing['LotFrontage'] = pd.to_numeric(housing['LotFrontage'], errors='coerce')
null_cols = housing.columns[housing.isnull().any()]
null_cols
for i in null_cols:
  if housing[i].dtype == np.float64 or housing[i].dtype == np.int64:
    housing[i].fillna(housing[i].mean(), inplace=True)
  else:
    housing[i].fillna(housing[i].mode()[0], inplace=True)
housing.isnull().sum()/housing.shape[0]*100

# @title Step 3: EDA

cat_cols = housing.select_dtypes(include='object').columns
cat_cols
num_cols = housing.select_dtypes(include=['int64', 'float64']).columns
num_cols

# @title Univariate Analysis

for i in num_cols:
  plt.figure(figsize=(8,5))
  print(i)
  sns.boxplot(housing[i])
  plt.show()
for i in num_cols:
    print(housing[i].value_counts(normalize=True))
    plt.figure(figsize=[6, 6])
    housing[i].value_counts(normalize=True).plot.pie(labeldistance=None, autopct='%1.2f%%')
    plt.legend()
    plt.show()

# @title Bivriate/Multivariate Analysis on the Dataset

sns.barplot(x='MSZoning', y='LotFrontage', data=housing)
plt.show()
sns.barplot(x='MSSubClass', y='LotFrontage', data=housing)
plt.show()
sns.barplot(x='HouseStyle',y='SalePrice',hue='Street',data=housing)
plt.show()
sns.barplot(x='BldgType',y='SalePrice',hue='Street',data=housing)
plt.show()

# @title Conclusion

housing["Age"] = housing["YrSold"] - housing["YearBuilt"]
housing["Age"].head()
housing.drop(columns=["YearBuilt", "YrSold"], axis=1, inplace = True)

# @title Correlation between Numerical Columns

plt.figure(figsize=[25, 25])
sns.heatmap(housing.corr(numeric_only=True), annot=True, cmap='BuPu')
plt.title("Correlation of Numerical Values")
plt.show()
k=10
plt.figure(figsize=[15,15])
cols = housing.corr(numeric_only=True).nlargest(13, 'SalePrice')['SalePrice'].index
cm = np.corrcoef(housing[cols].values.T)
hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)
plt.show()
cols = ["SalePrice", "OverallQual", "GrLivArea", "GarageCars", "TotalBsmtSF", "FullBath", "Age"]
plt.figure(figsize=[20,20])
sns.pairplot(housing[cols])
plt.show()

# @title Step 4: Data Preparation

housing_num = housing.select_dtypes(include=['int64', 'float64'])
housing_cat = housing.select_dtypes(include='object')
housing_cat
housing_cat_dm = pd.get_dummies(housing_cat, drop_first= True, dtype = int)
housing_cat_dm
house = pd.concat([housing_num, housing_cat_dm], axis = 1)
house.head()
house.shape
x = house.drop(['SalePrice'], axis = 1).copy()
y = house["SalePrice"].copy()
x.head()
y.head()
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state=42)
X_train.shape
X_test.shape
num_cols = list(X_train.select_dtypes(include=['int64', 'float64']).columns)
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])
def eval_metrics(y_train, y_train_pred, y_test, y_pred):
    print("r2 score (Train) =", '%.2f' % r2_score(y_train, y_train_pred))
    print("r2 score (Test) =", '%.2f' % r2_score(y_test, y_pred))
    
    mse_train = mean_squared_error(y_train, y_train_pred)
    mse_test = mean_squared_error(y_test, y_pred)
    rmse_train = mse_train**0.5
    rmse_test = mse_test**0.5
    
    print("RMSE (Train) =", '%.2f' % rmse_train) 
    print("RMSE (Test) =", '%.2f' % rmse_test)

# @title Step 5: Build the ML Model

import sklearn
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Ridge, Lasso
from sklearn.feature_selection import RFE
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error

params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0,
                    2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000]}

ridge = Ridge()

# cross validation

ridgeCV = GridSearchCV(estimator = ridge, 
                        param_grid = params, 
                        scoring= 'neg_mean_absolute_error',  
                        cv = 5, 
                        return_train_score=True,
                        verbose = 1, n_jobs=-1)            
ridgeCV.fit(X_train, y_train) 
ridgeCV.best_params_
ridgeCV.cv_results_
ridge = Ridge(alpha=9)
ridge.fit(X_train, y_train)
y_train_pred = ridge.predict(X_train)
y_pred = ridge.predict(X_test)
eval_metrics(y_train, y_train_pred, y_test, y_pred)
ridgeCV_res= pd.DataFrame(ridgeCV.cv_results_)
ridgeCV_res.head()
plt.plot(ridgeCV_res['param_alpha'], ridgeCV_res['mean_train_score'], label='Train')
plt.plot(ridgeCV_res['param_alpha'], ridgeCV_res['mean_test_score'], label='Test')
plt.xlabel('alpha')
plt.ylabel('R2_score')
plt.xscale('log')
plt.legend()
plt.show()
params = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0,
                    2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000]}

lasso = Lasso()

# cross validation

lassoCV = GridSearchCV(estimator = lasso, 
                        param_grid = params, 
                        scoring= 'neg_mean_absolute_error',  
                        cv = 5, 
                        return_train_score=True,
                        verbose = 1, n_jobs=-1)            
lassoCV.fit(X_train, y_train) 
lassoCV.best_params_
lasso = Lasso(alpha=0.001)
lasso.fit(X_train, y_train)
y_train_pred = lasso.predict(X_train)
y_pred = lasso.predict(X_test)
eval_metrics(y_train, y_train_pred, y_test, y_pred)
lassoCV_results= pd.DataFrame(lassoCV.cv_results_)
lassoCV_results.head()
plt.plot(lassoCV_results['param_alpha'], lassoCV_results['mean_train_score'], label='Train')
plt.plot(lassoCV_results['param_alpha'], lassoCV_results['mean_test_score'], label='Test')
plt.xlabel('alpha')
plt.ylabel('R2_score')
plt.xscale('log')
plt.legend()
plt.show()
betas = pd.DataFrame(index=x.columns)
betas.rows = x.columns
betas['Ridge'] = ridge.coef_
betas['Lasso'] = lasso.coef_
betas 
lasso_cols_removed = list(betas[betas['Lasso']==0].index)
print(lasso_cols_removed)
print(len(lasso_cols_removed))
lasso_cols_selected = list(betas[betas['Lasso']!=0].index)
print(lasso_cols_selected)
print(len(lasso_cols_selected))
betas['Ridge'].sort_values(ascending=False)[:10]
ridge_coeffs = np.exp(betas['Ridge'])
ridge_coeffs.sort_values(ascending=False)[:10]
betas['Lasso'].sort_values(ascending=False)[:10]
